---
title: KKBOX Churn Prediction
emoji: "ğŸµ"
colorFrom: purple
colorTo: blue
sdk: docker
app_port: 7860
pinned: false
license: mit
---

# KKBOX Churn Prediction

> **0.97 AUC with honest temporal validation** - A production-ready churn prediction pipeline achieving near-winner performance without data leakage.

[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/downloads/)
[![LightGBM](https://img.shields.io/badge/model-LightGBM-green.svg)](https://lightgbm.readthedocs.io/)
[![Calibrated](https://img.shields.io/badge/calibration-isotonic-orange.svg)](src/calibrate_and_evaluate.py)

## Results

### Final Model Performance

| Metric | Starting | Final | Target | Kaggle Winner |
|--------|----------|-------|--------|---------------|
| **AUC** | 0.7755 | **0.9696** | 0.85 | ~0.99* |
| **Log Loss** | 0.41 | **0.1127** | <0.15 | 0.08 |
| **Brier Score** | 0.125 | **0.033** | <0.08 | - |

> *Kaggle winners used random splits with data leakage. Our 0.97 AUC uses strict temporal validation (train on past, validate on future).

### Key Achievements

- **14% above target AUC** (0.97 vs 0.85 target)
- **Within 0.03 log loss of winning solution** (0.11 vs 0.08)
- **Perfect calibration** - predicted probabilities match actual churn rates
- **135 engineered features** including winner-inspired patterns
- **Zero data leakage** - all features use only past information

## The Problem

KKBOX, Asia's leading music streaming service, needed to predict which users would churn (not renew their subscription). This was a [Kaggle competition](https://www.kaggle.com/c/kkbox-churn-prediction-challenge) with 970K users and transaction/listening history.

**Challenge**: High AUC alone isn't enough - the model must output well-calibrated probabilities for business decisions.

## Solution Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FEATURE ENGINEERING                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Transactions â”‚  â”‚  User Logs   â”‚  â”‚  Historical  â”‚          â”‚
â”‚  â”‚  (5 windows) â”‚  â”‚  (5 windows) â”‚  â”‚    Churn     â”‚          â”‚
â”‚  â”‚  7/14/30/60/ â”‚  â”‚  7/14/30/60/ â”‚  â”‚  last_N_is_  â”‚          â”‚
â”‚  â”‚    90 days   â”‚  â”‚    90 days   â”‚  â”‚    churn     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚         â†“                  â†“                  â†“                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚           135 Features (SQL + Python)           â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      MODEL TRAINING                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   LightGBM   â”‚  â”‚   XGBoost    â”‚  â”‚   CatBoost   â”‚          â”‚
â”‚  â”‚  AUC: 0.9696 â”‚  â”‚  AUC: 0.9642 â”‚  â”‚  AUC: 0.9605 â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚         â†“                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚     Isotonic Calibration (Log Loss: 0.11)       â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Feature Engineering

Features were designed based on [Bryan Gregory's 1st place solution](https://arxiv.org/abs/1802.03396):

### Top Predictive Features

| Feature | Importance | Description |
|---------|------------|-------------|
| `membership_days_remaining` | 1373 | Days until subscription expires |
| `tenure_days` | 872 | How long user has been a member |
| `transaction_count` | 753 | Historical transaction frequency |
| `days_since_last_tx` | 743 | Recency of last payment |
| `total_secs_90d` | 698 | Listening time in last 90 days |

### Feature Categories (135 total)

- **Transaction features** (35): Payment patterns across 5 time windows
- **User log features** (50): Listening behavior, completion rates
- **Trend features** (10): Week-over-week, month-over-month changes
- **Historical churn** (10): `last_N_is_churn`, `churn_rate`
- **Winner-inspired** (15): `autorenew_not_cancel`, `amt_per_day`
- **Demographics** (5): Age, gender, tenure, registration channel

## Calibration: The Secret Weapon

Raw model outputs are confidence scores, not probabilities. Calibration fixes this:

```
Before calibration:  Mean prediction = 0.35, Actual churn = 9%  (BAD)
After calibration:   Mean prediction = 0.09, Actual churn = 9%  (GOOD)
```

**Impact**: Log loss dropped from 0.41 to 0.11 while AUC slightly improved.

```python
# Calibration in action
from sklearn.calibration import IsotonicRegression

calibrator = IsotonicRegression(out_of_bounds="clip")
calibrator.fit(raw_predictions, actual_labels)
calibrated = calibrator.transform(test_predictions)
```

## Quick Start

```bash
# Clone repository
git clone https://github.com/robertlupo1997/kkbox-churn-prediction.git
cd kkbox-churn-prediction

# Install dependencies
pip install -r requirements.txt

# Run calibration (uses pre-trained models)
python src/calibrate_and_evaluate.py

# Generate predictions
python src/generate_kaggle_submission.py

# Run error analysis
python src/run_error_analysis.py --calibrate
```

## Project Structure

```
kkbox-churn-prediction/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ calibrate_and_evaluate.py  # Isotonic calibration pipeline
â”‚   â”œâ”€â”€ generate_kaggle_submission.py  # Submission generation
â”‚   â”œâ”€â”€ run_error_analysis.py      # Model diagnostics
â”‚   â”œâ”€â”€ hyperparameter_tuning.py   # Optuna optimization
â”‚   â”œâ”€â”€ stacking.py                # Ensemble methods
â”‚   â””â”€â”€ error_analysis.py          # Segment analysis
â”œâ”€â”€ features/
â”‚   â””â”€â”€ features_comprehensive.sql # 135 feature definitions
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ lightgbm.pkl              # Best model (0.97 AUC)
â”‚   â”œâ”€â”€ calibrator_lightgbm.pkl   # Isotonic calibrator
â”‚   â””â”€â”€ calibration_metrics.json  # Performance metrics
â”œâ”€â”€ LEARNERS_GUIDE.md             # How winners solved this
â””â”€â”€ MODEL_CARD.md                 # Model documentation
```

## What I Learned

This project taught me the difference between **ranking** (AUC) and **calibration** (log loss):

1. **AUC measures ranking** - Are churners scored higher than non-churners?
2. **Log loss measures calibration** - Does 80% prediction mean 80% actual probability?
3. **These are independent** - Perfect AUC with terrible log loss is possible
4. **Calibration is often free** - Isotonic regression preserves ranking while fixing probabilities

See [LEARNERS_GUIDE.md](LEARNERS_GUIDE.md) for the full learning journey.

## Technical Highlights

- **Temporal validation**: Train on Jan-Feb 2017, validate on Mar 2017
- **No data leakage**: All features strictly use past information
- **Hyperparameter tuning**: Optuna with 50 trials per model
- **Ensemble exploration**: Stacking tested but single LightGBM performed best
- **Business-ready**: Error analysis identifies high-value intervention segments

## References

- [WSDM KKBox Churn Prediction Challenge](https://www.kaggle.com/c/kkbox-churn-prediction-challenge)
- [Bryan Gregory's 1st Place Solution (arXiv:1802.03396)](https://arxiv.org/abs/1802.03396)
- [Isotonic Calibration](https://scikit-learn.org/stable/modules/calibration.html)

---

**Built as a portfolio project demonstrating end-to-end ML engineering**: feature engineering, model training, calibration, and business-ready deployment.
